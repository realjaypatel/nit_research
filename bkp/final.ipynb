{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from output.csv\n",
    "\n",
    "data = pd.read_csv('../output/output.csv')\n",
    "\n",
    "# Split features and labels\n",
    "feature_df = data.drop(data.columns[:2], axis=1)\n",
    "X = np.asarray(feature_df)\n",
    "y = np.asarray(data['pii_exist'])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_scaled, y, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "# Define privacy parameters\n",
    "epsilon = 1000  # Privacy budget\n",
    "delta = 1e-6  # Desired overall privacy failure probability\n",
    "\n",
    "# Define SVM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define a function for training on each client's data with differential privacy\n",
    "def train_on_client_dp(X, y, model, epsilon, delta):\n",
    "    # Compute the scale parameter for Gaussian noise\n",
    "    delta_prime = delta / (2 * len(X) / 32)  # Assuming batch size of 32\n",
    "    c = np.sqrt(2 * np.log(1.25 / delta_prime))\n",
    "    sensitivity = 2 * c\n",
    "    sigma = c * sensitivity / epsilon\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Add noise to the gradients\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            for weight in layer.trainable_variables:\n",
    "                noise = tf.random.normal(shape=weight.shape, stddev=sigma)\n",
    "                weight.assign_add(noise)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Federated learning loop with differential privacy\n",
    "global_model = tf.keras.models.clone_model(model)  # Create a copy of the original model\n",
    "noclient = 10\n",
    "for i in range(noclient):  # 20 clients\n",
    "    # Divide the training data into 10 parts\n",
    "    start_index = int(i * len(X_train) / noclient)\n",
    "    end_index = int((i + 1) * len(X_train) / noclient)\n",
    "    X_client_train = X_train[start_index:end_index]\n",
    "    y_client_train = y_train_one_hot[start_index:end_index]\n",
    "\n",
    "    # Train client model on its data with differential privacy\n",
    "    client_model = train_on_client_dp(X_client_train, y_client_train, global_model, epsilon, delta)\n",
    "\n",
    "    # Aggregate weights of the client model onto the global model\n",
    "    for global_layer, client_layer in zip(global_model.layers, client_model.layers):\n",
    "        global_layer_weights = global_layer.get_weights()\n",
    "        client_layer_weights = client_layer.get_weights()\n",
    "        aggregated_weights = [(w1 + w2) / 2 for w1, w2 in zip(global_layer_weights, client_layer_weights)]\n",
    "        global_layer.set_weights(aggregated_weights)\n",
    "\n",
    "    # Evaluate client model on the test set\n",
    "    y_pred = np.argmax(client_model.predict(X_test), axis=1)\n",
    "    yg_pred = np.argmax(global_model.predict(Xg_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1g = f1_score(yg_test, yg_pred, average='weighted')\n",
    "    print(\"Client\", i+1, \"F1 Score:\", f1,\"F1 Global\",f1g)\n",
    "\n",
    "\n",
    "# Predict classes using the global model\n",
    "y_pred = np.argmax(global_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate confusion matrix and F1 score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 1 F1 Score: 0.6956970621783001 F1 Global 0.7146790608953445\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 2 F1 Score: 0.7858105034723117 F1 Global 0.8018835229129663\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Client 3 F1 Score: 0.7504240916480105 F1 Global 0.7534323367427223\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 4 F1 Score: 0.774207126125151 F1 Global 0.787744786822341\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Client 5 F1 Score: 0.8792817367440144 F1 Global 0.8934167607519131\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Client 6 F1 Score: 0.7885060877450318 F1 Global 0.8035898040139821\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 7 F1 Score: 0.9041882414300652 F1 Global 0.9291423985691085\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 8 F1 Score: 0.8980957809299891 F1 Global 0.9149100893221532\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "Client 9 F1 Score: 0.8962039282159039 F1 Global 0.9283524685363949\n",
      "16/16 [==============================] - 0s 1ms/step\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Client 10 F1 Score: 0.9177294824134973 F1 Global 0.926618422804901\n",
      "16/16 [==============================] - 0s 2ms/step\n",
      "Confusion Matrix:\n",
      "[[337  26]\n",
      " [ 15 114]]\n",
      "F1 Score: 0.9177294824134973\n"
     ]
    }
   ],
   "source": [
    "#final with epoch 1000 f1score: 0.95\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from output.csv\n",
    "\n",
    "data = pd.read_csv('../output/output.csv')\n",
    "\n",
    "# Split features and labels\n",
    "feature_df = data.drop(data.columns[:2], axis=1)\n",
    "X = np.asarray(feature_df)\n",
    "y = np.asarray(data['pii_exist'])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_scaled, y, test_size=0.2, random_state=101)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "# Define privacy parameters\n",
    "epsilon = 100  # Privacy budget\n",
    "delta = 1e-2  # Desired overall privacy failure probability\n",
    "\n",
    "# Create a simple MLP model for classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define a function for training on each client's data with differential privacy\n",
    "def train_on_client_dp(X, y, model, epsilon, delta):\n",
    "    # Compute the scale parameter for Gaussian noise\n",
    "    delta_prime = delta / (2 * len(X) / 32)  # Assuming batch size of 32\n",
    "    c = np.sqrt(2 * np.log(1.25 / delta_prime))\n",
    "    sensitivity = 2 * c\n",
    "    sigma = c * sensitivity / epsilon\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Add noise to the gradients\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            for weight in layer.trainable_variables:\n",
    "                noise = tf.random.normal(shape=weight.shape, stddev=sigma)\n",
    "                weight.assign_add(noise)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Federated learning loop with differential privacy\n",
    "global_model = tf.keras.models.clone_model(model)  # Create a copy of the original model\n",
    "noclient = 10\n",
    "for i in range(noclient):  # 20 clients\n",
    "    # Divide the training data into 10 parts\n",
    "    start_index = int(i * len(X_train) / noclient)\n",
    "    end_index = int((i + 1) * len(X_train) / noclient)\n",
    "    X_client_train = X_train[start_index:end_index]\n",
    "    y_client_train = y_train_one_hot[start_index:end_index]\n",
    "\n",
    "    # Train client model on its data with differential privacy\n",
    "    client_model = train_on_client_dp(X_client_train, y_client_train, global_model, epsilon, delta)\n",
    "\n",
    "    # Aggregate weights of the client model onto the global model\n",
    "    for global_layer, client_layer in zip(global_model.layers, client_model.layers):\n",
    "        global_layer_weights = global_layer.get_weights()\n",
    "        client_layer_weights = client_layer.get_weights()\n",
    "        aggregated_weights = [(w1 + w2) / 2 for w1, w2 in zip(global_layer_weights, client_layer_weights)]\n",
    "        global_layer.set_weights(aggregated_weights)\n",
    "\n",
    "    # Evaluate client model on the test set\n",
    "    y_pred = np.argmax(client_model.predict(X_test), axis=1)\n",
    "    yg_pred = np.argmax(global_model.predict(Xg_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1g = f1_score(yg_test, yg_pred, average='weighted')\n",
    "    print(\"Client\", i+1, \"F1 Score:\", f1,\"F1 Global\",f1g)\n",
    "\n",
    "\n",
    "# Predict classes using the global model\n",
    "y_pred = np.argmax(global_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate confusion matrix and F1 score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/#create=1&folderId=10t9mdGTqNlaVHNGIIp281bcfh30cdPOZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bina differential privacy ke : f1score: 0.95\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from output.csv\n",
    "\n",
    "data = pd.read_csv('../output/output.csv')\n",
    "\n",
    "# Split features and labels\n",
    "feature_df = data.drop(data.columns[:2], axis=1)\n",
    "X = np.asarray(feature_df)\n",
    "y = np.asarray(data['pii_exist'])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_scaled, y, test_size=0.1, random_state=101)\n",
    "\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "# Define privacy parameters\n",
    "\n",
    "# Define SVM model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define a function for training on each client's data with differential privacy\n",
    "def train_on_client_dp(X, y, model):\n",
    "\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "# Federated learning loop with differential privacy\n",
    "global_model = tf.keras.models.clone_model(model)  # Create a copy of the original model\n",
    "noclient = 10\n",
    "for i in range(noclient):  # 20 clients\n",
    "    # Divide the training data into 10 parts\n",
    "    start_index = int(i * len(X_train) / noclient)\n",
    "    end_index = int((i + 1) * len(X_train) / noclient)\n",
    "    X_client_train = X_train[start_index:end_index]\n",
    "    y_client_train = y_train_one_hot[start_index:end_index]\n",
    "\n",
    "    # Train client model on its data with differential privacy\n",
    "    client_model = train_on_client_dp(X_client_train, y_client_train, global_model)\n",
    "\n",
    "    # Aggregate weights of the client model onto the global model\n",
    "    for global_layer, client_layer in zip(global_model.layers, client_model.layers):\n",
    "        global_layer_weights = global_layer.get_weights()\n",
    "        client_layer_weights = client_layer.get_weights()\n",
    "        aggregated_weights = [(w1 + w2) / 2 for w1, w2 in zip(global_layer_weights, client_layer_weights)]\n",
    "        global_layer.set_weights(aggregated_weights)\n",
    "\n",
    "    # Evaluate client model on the test set\n",
    "    y_pred = np.argmax(client_model.predict(X_test), axis=1)\n",
    "    yg_pred = np.argmax(global_model.predict(Xg_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1g = f1_score(yg_test, yg_pred, average='weighted')\n",
    "    print(\"Client\", i+1, \"F1 Score:\", f1,\"F1 Global\",f1g)\n",
    "\n",
    "\n",
    "# Predict classes using the global model\n",
    "y_pred = np.argmax(global_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate confusion matrix and F1 score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"Confusion Matrix:\")\n",
    "\n",
    "print(conf_matrix)\n",
    "\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 1 F1 Score: 0.6711565714859437 F1 Global 0.7115745284947513\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "Client 2 F1 Score: 0.792791104273026 F1 Global 0.8061065740623753\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "Client 3 F1 Score: 0.8533691652049389 F1 Global 0.8586992927201155\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 4 F1 Score: 0.8629762502690679 F1 Global 0.8710046857957257\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 5 F1 Score: 0.8863817043354489 F1 Global 0.8813063044885525\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "Client 6 F1 Score: 0.8867901562898303 F1 Global 0.8971321305567453\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 7 F1 Score: 0.7577639608121605 F1 Global 0.7773007731873723\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 8 F1 Score: 0.7693367956861369 F1 Global 0.7937781319846768\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 9 F1 Score: 0.9020461955897532 F1 Global 0.9305744443752815\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "12/12 [==============================] - 0s 4ms/step\n",
      "Client 10 F1 Score: 0.9044005068800167 F1 Global 0.9096473735212135\n",
      "12/12 [==============================] - 0s 5ms/step\n",
      "Confusion Matrix:\n",
      "[[238  32]\n",
      " [  4  88]]\n",
      "F1 Score: 0.9044005068800167\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from output.csv\n",
    "data = pd.read_csv('/media/jay/Windows/Users/jay/Downloads/nit_research/output/output_1.csv')\n",
    "\n",
    "X = np.asarray(data.drop(data.columns[:2], axis=1))\n",
    "y = np.asarray(data['pii_exist'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "Xg_train, Xg_test, yg_train, yg_test = train_test_split(X_scaled, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(y))\n",
    "y_train_one_hot = tf.one_hot(y_train, depth=num_classes).numpy()\n",
    "y_test_one_hot = tf.one_hot(y_test, depth=num_classes).numpy()\n",
    "\n",
    "# Define privacy parameters\n",
    "epsilon = 100  # Privacy budget\n",
    "delta = 1e-2   # Desired overall privacy failure probability\n",
    "\n",
    "# Create a simple MLP model for classification\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Define a function for training on each client's data with differential privacy\n",
    "def train_on_client_dp(X, y, epsilon, delta):\n",
    "    # Compute the scale parameter for Gaussian noise\n",
    "    delta_prime = delta / (2 * len(X) / 32)  # Assuming batch size of 32\n",
    "    c = np.sqrt(2 * np.log(1.25 / delta_prime))\n",
    "    sensitivity = 2 * c\n",
    "    sigma = c * sensitivity / epsilon\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "\n",
    "    # Add noise to the gradients\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Dense):\n",
    "            for weight in layer.trainable_variables:\n",
    "                noise = tf.random.normal(shape=weight.shape, stddev=sigma)\n",
    "                weight.assign_add(noise)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define the global aggregator function\n",
    "def global_aggregator(global_model, client_model):\n",
    "    \"\"\"\n",
    "    Aggregate weights of the client model onto the global model.\n",
    "\n",
    "    Parameters:\n",
    "        global_model (tf.keras.Model): Global model to be updated.\n",
    "        client_model (tf.keras.Model): Client model whose weights are to be aggregated onto the global model.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Updated global model with aggregated weights.\n",
    "    \"\"\"\n",
    "    for global_layer, client_layer in zip(global_model.layers, client_model.layers):\n",
    "        global_layer_weights = global_layer.get_weights()\n",
    "        client_layer_weights = client_layer.get_weights()\n",
    "        aggregated_weights = [(w1 + w2) / 2 for w1, w2 in zip(global_layer_weights, client_layer_weights)]\n",
    "        global_layer.set_weights(aggregated_weights)\n",
    "\n",
    "    return global_model\n",
    "\n",
    "# Federated learning loop with differential privacy\n",
    "global_model = tf.keras.models.clone_model(model)  # Create a copy of the original model\n",
    "noclient = 10\n",
    "for i in range(noclient):  # 10 clients\n",
    "    # Divide the training data into parts for each client\n",
    "    start_index = int(i * len(X_train) / noclient)\n",
    "    end_index = int((i + 1) * len(X_train) / noclient)\n",
    "    X_client_train = X_train[start_index:end_index]\n",
    "    y_client_train = y_train_one_hot[start_index:end_index]\n",
    "\n",
    "    # Train client model on its data with differential privacy\n",
    "    client_model = train_on_client_dp(X_client_train, y_client_train, epsilon, delta)\n",
    "\n",
    "    # Aggregate weights of the client model onto the global model\n",
    "    global_model = global_aggregator(global_model, client_model)\n",
    "\n",
    "    # Evaluate client model on the test set\n",
    "    y_pred = np.argmax(client_model.predict(X_test), axis=1)\n",
    "    yg_pred = np.argmax(global_model.predict(Xg_test), axis=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    f1g = f1_score(yg_test, yg_pred, average='weighted')\n",
    "    print(\"Client\", i+1, \"F1 Score:\", f1, \"F1 Global\", f1g)\n",
    "\n",
    "# Predict classes using the global model\n",
    "y_pred = np.argmax(global_model.predict(X_test), axis=1)\n",
    "\n",
    "# Calculate confusion matrix and F1 score\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
